{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from decord import VideoReader\n",
    "from moviepy.editor import AudioFileClip\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "\n",
    "input_size = 224\n",
    "num_frame = 8\n",
    "sampling_rate = 6\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio\n",
    "\n",
    "def MFCC(signal, sample_rate):\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.0001\n",
    "\n",
    "    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    NFFT = 512\n",
    "\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "    nfilt = 40\n",
    "\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "    num_ceps = 13\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm=\"ortho\")[:, 1: (num_ceps + 1)]\n",
    "    cep_lifter = 22\n",
    "    (nframes, ncoeff) = mfcc.shape\n",
    "    n = np.arange(ncoeff)\n",
    "    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
    "    mfcc *= lift\n",
    "    return mfcc\n",
    "\n",
    "def read_video(file_path):\n",
    "    vr = VideoReader(file_path)\n",
    "    frames = vr.get_batch(range(len(vr))).asnumpy()\n",
    "    return format_frames(frames, output_size=(input_size, input_size))\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.uint8)\n",
    "    frame = tf.image.resize(frame, size=list(output_size))\n",
    "    return frame\n",
    "\n",
    "def uniform_temporal_subsample(x, num_samples, clip_idx, total_clips, frame_rate=1, temporal_dim=-4):\n",
    "    t = tf.shape(x)[temporal_dim]\n",
    "    max_offset = t - num_samples * frame_rate\n",
    "    step = max_offset // total_clips\n",
    "    offset = clip_idx * step\n",
    "    indices = tf.linspace(\n",
    "        tf.cast(offset, tf.float32),\n",
    "        tf.cast(offset + (num_samples - 1) * frame_rate, tf.float32),\n",
    "        num_samples\n",
    "    )\n",
    "    indices = tf.clip_by_value(indices, 0, tf.cast(t - 1, tf.float32))\n",
    "    indices = tf.cast(tf.round(indices), tf.int32)\n",
    "    return tf.gather(x, indices, axis=temporal_dim)\n",
    "\n",
    "def clip_generator(image, num_frames=32, frame_rate=1, num_clips=1, crop_size=224):\n",
    "    clips_list = []\n",
    "    for i in range(num_clips):\n",
    "        frame = uniform_temporal_subsample(\n",
    "            image, num_frames, i, num_clips, frame_rate=frame_rate, temporal_dim=0\n",
    "        )\n",
    "        clips_list.append(frame)\n",
    "\n",
    "    video = tf.stack(clips_list)\n",
    "    video = tf.reshape(video, [num_clips * num_frames, crop_size, crop_size, 3])\n",
    "    return video\n",
    "\n",
    "def video_audio(input_path, output_path):\n",
    "    n = 0\n",
    "\n",
    "    for video_file in os.listdir(input_path):\n",
    "        if video_file.endswith('.avi'):  \n",
    "            video_path = os.path.join(input_path, video_file)\n",
    "\n",
    "            mp4_name = video_file[:-4] + '.mp4'\n",
    "            output_video_path = os.path.join(output_path, mp4_name)\n",
    "\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            output_video = cv2.VideoWriter(output_video_path, fourcc, 16.0, (224, 224))\n",
    "\n",
    "            video_ds = read_video(video_path)\n",
    "            video_ds = clip_generator(video_ds, num_frame, sampling_rate, num_clips=1)\n",
    "\n",
    "            audio_clip = AudioFileClip(video_path)\n",
    "            wave_name = video_file[:-4] + '.wav'\n",
    "            #path_audio_save = os.path.join('Data\\\\MEAD\\\\MEAD_WAVE', wave_name)\n",
    "\n",
    "            audio_clip.write_audiofile(path_audio_save)\n",
    "            fs, Audiodata = wavfile.read(path_audio_save)\n",
    "            Audiodata = normalize_audio(Audiodata)\n",
    "            step = int(len(Audiodata) / 9) - 1\n",
    "            tx = np.arange(0, len(Audiodata), step)\n",
    "\n",
    "            for i in range(8):\n",
    "                signal = Audiodata[tx[i]:tx[i + 2]]\n",
    "                mfcc = MFCC(signal, fs)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(4, 4))\n",
    "                ax.matshow(np.transpose(mfcc), interpolation=\"nearest\", aspect=\"auto\", origin=\"lower\")\n",
    "                plt.axis('off')\n",
    "                plt.savefig(\"MFCC.jpg\")\n",
    "                plt.close(fig)\n",
    "\n",
    "                audio_img = Image.open(\"MFCC.jpg\").resize((224, 224))\n",
    "                audio_img = np.array(audio_img)\n",
    "                output_video.write(audio_img.astype('uint8'))\n",
    "\n",
    "            # Add video frames\n",
    "            for i in range(8):\n",
    "                video_img = video_ds.numpy()[i].astype('uint8')\n",
    "                output_video.write(video_img)\n",
    "\n",
    "            output_video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    #path_input = 'D:/enterface/all_avi_files'\n",
    "    #save_output_path = 'D:/enterface/mfccAndVideoframes'\n",
    "\n",
    "    #n_processed = video_audio(path_input, save_output_path)\n",
    "\n",
    "    #print(f\"Total processed files: {n_processed}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
